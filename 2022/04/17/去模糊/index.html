<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/images/icons/favicon-16x16.png?v=2.1.1" type="image/png" sizes="16x16"><link rel="icon" href="/images/icons/favicon-32x32.png?v=2.1.1" type="image/png" sizes="32x32"><meta name="description" content="题目介绍        用深度傅里叶变换把模糊图像清晰化                       项目成员        智能 1902 庞宇桐 1930330075 智能 1902 马艺雯 1930330074 智能 1902 彭梓钰 1930330076 智能 1902 涂泓婧 1930330077                       项目进">
<meta property="og:type" content="article">
<meta property="og:title" content="第四组 图像去模糊--De-blurring">
<meta property="og:url" content="http://zhineng1902.github.io/2022/04/17/%E5%8E%BB%E6%A8%A1%E7%B3%8A/index.html">
<meta property="og:site_name" content="智能课程设计">
<meta property="og:description" content="题目介绍        用深度傅里叶变换把模糊图像清晰化                       项目成员        智能 1902 庞宇桐 1930330075 智能 1902 马艺雯 1930330074 智能 1902 彭梓钰 1930330076 智能 1902 涂泓婧 1930330077                       项目进">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://zhineng1902.github.io/assets/4.%E4%BB%A3%E7%A0%81.jpg">
<meta property="og:image" content="http://zhineng1902.github.io/assets/4.%E4%BB%A3%E7%A0%812.jpg">
<meta property="og:image" content="http://zhineng1902.github.io/assets/4.%E4%BB%A3%E7%A0%813.jpg">
<meta property="og:image" content="http://zhineng1902.github.io/assets/4.%E4%BB%A3%E7%A0%814.jpg">
<meta property="og:image" content="http://zhineng1902.github.io/assets/4.%E4%BB%A3%E7%A0%815.jpg">
<meta property="og:image" content="http://zhineng1902.github.io/assets/4.%E4%BB%A3%E7%A0%816.jpg">
<meta property="article:published_time" content="2022-04-17T06:00:00.000Z">
<meta property="article:modified_time" content="2022-04-19T02:19:22.263Z">
<meta property="article:author" content="AndiXing LihuaYao YuchenZhou">
<meta property="article:tag" content="python">
<meta property="article:tag" content="去模糊">
<meta property="article:tag" content="傅里叶变换">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://zhineng1902.github.io/assets/4.%E4%BB%A3%E7%A0%81.jpg"><title>第四组 图像去模糊--De-blurring | 智能课程设计</title><link ref="canonical" href="http://zhineng1902.github.io/2022/04/17/%E5%8E%BB%E6%A8%A1%E7%B3%8A/"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.1.1"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":true,"scrollDownIcon":false},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"default","highlight":"light","wordWrap":false},
  reward: false,
  fancybox: false,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: false,
  pjax: undefined,
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"复制","copySuccess":"复制成功","copyError":"复制失败"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 5.4.2"></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner"><nav class="header-nav header-nav--fixed"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">首页</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/archives/"><span class="header-nav-menu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-menu-item__text">归档</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/categories/"><span class="header-nav-menu-item__icon"><i class="fas fa-layer-group"></i></span><span class="header-nav-menu-item__text">分类</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/tags/"><span class="header-nav-menu-item__icon"><i class="fas fa-tags"></i></span><span class="header-nav-menu-item__text">标签</span></a></div></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav><div class="header-banner"><div class="header-banner-info"><div class="header-banner-info__title">智能课程设计</div><div class="header-banner-info__subtitle">1902班</div></div></div></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content" id="content"><!-- Just used to judge whether it is an article page--><div id="is-post"></div><div class="post"><header class="post-header"><h1 class="post-title">第四组 图像去模糊--De-blurring</h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2022-04-17</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2022-04-19</span></span></div></header><div class="post-body">
        <h1 id="题目介绍"   >
          <a href="#题目介绍" class="heading-link"><i class="fas fa-link"></i></a>题目介绍</h1>
      <ul>
<li>用深度傅里叶变换把模糊图像清晰化</li>
</ul>
<hr>

        <h1 id="项目成员"   >
          <a href="#项目成员" class="heading-link"><i class="fas fa-link"></i></a>项目成员</h1>
      <ul>
<li>智能 1902 庞宇桐 1930330075</li>
<li>智能 1902 马艺雯 1930330074</li>
<li>智能 1902 彭梓钰 1930330076</li>
<li>智能 1902 涂泓婧 1930330077</li>
</ul>
<hr>

        <h1 id="项目进度"   >
          <a href="#项目进度" class="heading-link"><i class="fas fa-link"></i></a>项目进度</h1>
      
        <h2 id="第一周"   >
          <a href="#第一周" class="heading-link"><i class="fas fa-link"></i></a>第一周:</h2>
      <ul>
<li>看过的论文链接：<span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://arxiv.org/abs/2111.11745" >https://arxiv.org/abs/2111.11745</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
<li>在端到端图像去模糊架构中，通常会采用ResBlock来学习模糊和清晰图像对之间的区别。从模糊的图像中重建一个清晰的图像需要改变低频和高频信息。虽然传统的ResBlock可以很好的捕捉图像的高频分量，但它往往会忽略低频信息。此外，ResBlock通常不能有效地对长距离信息建模，这在从模糊图像中重建清晰图像时不是重要的。在本文中，我们提出了一种带有卷积块(ResFFT-Conv块)的残差快速傅里叶变换，能够捕获长期和短期交互，同时集成低频和高频残差信息。ResFFT-Conv块是一个概念简单但计算效率高的即插即用块，在不同的架构中获得了显著的性能提高。利用ResFFT-Conv块，我们进一步提出了一个基于MIMO-UNet的深度残差傅里叶变换(DeepRFT)框架，在GoPro、HIDE、RealBlur和DPDD数据集上实现了最先进的图像去模糊性能。实验表明，我们的DeepRFT可以显著提高图像去模糊性能(例如，与MIMO-UNPro数据集相比，PSNR的GoPro提高了1.09dB)，在GoPro数据集上，DeepRFT+的DeepR数据集的PSNR甚至达到33.23dB。观看的视频:<span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1P5411o7YH?p=18&spm_id_from=333.880.my_history.page.click" >https://www.bilibili.com/video/BV1P5411o7YH?p=18&amp;spm_id_from=333.880.my_history.page.click</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>
        <h2 id="第二周"   >
          <a href="#第二周" class="heading-link"><i class="fas fa-link"></i></a>第二周:</h2>
      </li>
<li>新看过论文链接[[1803.03363] Learning a Discriminative Prior for Blind Image Deblurring (arxiv.org)]([1803.03363] Learning a Discriminative Prior for Blind Image Deblurring (arxiv.org))</li>
<li>该论文提出了一种有效的基于数据驱动的判别先验的盲图像去模糊方法。这个工作是由一个事实，即一个好的图像优先应有利于清晰的图像模糊这httpURL 这项工作，作者制定的图像先验作为一个二进制分类器，可以实现由深度卷积神经网络（ CNN ），学习的先验是能够区分输入图像是否清晰。嵌入到最大后( MAP )支架，它有助于在各种场景下进行盲去模糊，包括自然图像、人脸图像、文本图像和低照度图像。然而，这是很难优化的去模糊方法与学习的图像先验，因为它涉及到一个非线性 CNN 。因此，作者开发了一个有效的数值方法的基础上的半二次分裂法和梯度下降算法来解决所提出的模型。此外，该模型可以很容易地扩展到非均匀去模糊。定性和定量的实验结果表明，作者的方法表现出良好的国家的最先进的算法，以及特定领域的图像去模糊方法。<br>观看的视频：<span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1P5411o7YH?p=18&spm_id_from=333.880.my_history.page.click" >https://www.bilibili.com/video/BV1P5411o7YH?p=18&amp;spm_id_from=333.880.my_history.page.click</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>
        <h2 id="第三周"   >
          <a href="#第三周" class="heading-link"><i class="fas fa-link"></i></a>第三周:</h2>
      </li>
<li>新增看过的论文链接<span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&dbname=CJFDLAST2020&filename=BZGC202015038&uniplatform=NZKPT&v=cq97cUWPWg6oWj2wMbRyJrNlU7lJw-dWZkJCYHL64D6tPimeCRMLNwQE_x-HGN4P" >https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&amp;dbname=CJFDLAST2020&amp;filename=BZGC202015038&amp;uniplatform=NZKPT&amp;v=cq97cUWPWg6oWj2wMbRyJrNlU7lJw-dWZkJCYHL64D6tPimeCRMLNwQE_x-HGN4P</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
<li>目的为了有效地去除多种图像模糊,提高图像质量,提出基于深度强化学习的图像去模糊方法。方法选用GoPro与DIV2K这2个数据集进行实验,以峰值信噪比（PSNR）和结构相似性（SSIM）为客观评价指标。通过卷积神经网络获得模糊图像的高维特征,利用深度强化学习结合多种CNN去模糊工具建立去模糊框架,将峰值信噪比（PSNR）作为训练奖励评价函数,来选择最优修复策略,逐步对模糊图像进行修复。结果通过训练与测试,与现有的主流算法相比,文中方法有着更好的主观视觉效果,且PSNR值与SSIM值都有更好的表现。结论实验结果表明,文中方法能有效地解决图像的高斯模糊和运动模糊等问题,并取得了良好的视觉效果,在图像去模糊领域具有一定的参考价值。 
        <h2 id="第四周"   >
          <a href="#第四周" class="heading-link"><i class="fas fa-link"></i></a>第四周</h2>
      </li>
<li>新看过的论文链接<span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFDTEMP&filename=1022430666.nh&uniplatform=NZKPT&v=I9Jz4EeHHfSOltADAc9KBv4-mW6ru-hT86-IrmhX6fDEetzE0tQv1meucpe2IO8V" >https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&amp;dbname=CMFDTEMP&amp;filename=1022430666.nh&amp;uniplatform=NZKPT&amp;v=I9Jz4EeHHfSOltADAc9KBv4-mW6ru-hT86-IrmhX6fDEetzE0tQv1meucpe2IO8V</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
<li>本文对单一运动图像盲去模糊算法进行了深入探索，通过大量对比实验验证了不同网络结构的表现。研究工作如下：（1）首先在整体架构方面，与以往的使用“由粗到精”的多尺度去模糊网络相比，本文设计了一个新的端到端的、可训练的多层次分块网络以快速高效得到不同空间尺度图像。优化的特征提取机制显著改善了特征提取过程，充分利用了图像原有的空间尺度信息并极大加速了多尺度特征提取过程。（2）针对以往去模糊方法效果较差的情况，提出在编码-解码过程中引入上下文机制从而使得多尺度特征更加丰富，从而有效利用全局信息和局部细节以提高网络处理不同程度模糊能力。（3）在对去模糊模型进行效果和效率方面的权衡后，本文适当增加了残差学习模块，进一步提升了网络去模糊效果。 相关实验结果表明，相较于以往方法，本文提出方法在不同标准数据集上均取得了较好效果。在图像质量方面，本文方法对模糊图像有不同程度的图像质量改善，无论是图像质量客观评价标准还是主观视觉感受都验证了这一点。在处理效率方面，得益于高效的特征提取模块，本文使用的方法运行速度较快且模型参数量较小。
        <h2 id="第五周"   >
          <a href="#第五周" class="heading-link"><i class="fas fa-link"></i></a>第五周</h2>
      </li>
<li>新看过的论文链接<span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD202201&filename=1021913386.nh&uniplatform=NZKPT&v=DBaduyTSrqfdgNtXMMam2mb1lZByEYLwULCybO2ZkQbZAHdOfkl64EaLVu8uf3W2" >https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&amp;dbname=CMFD202201&amp;filename=1021913386.nh&amp;uniplatform=NZKPT&amp;v=DBaduyTSrqfdgNtXMMam2mb1lZByEYLwULCybO2ZkQbZAHdOfkl64EaLVu8uf3W2</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
<li>该论文基于深度学习开展图像去模糊研究，主要研究工作如下：(1) 研究了一种基于生成对抗网络的单幅图像去模糊算法。本文使用 Cycle GAN对非成对数据即单幅图像进行端到端的盲去图像模糊，并拟定了合适的损失函数来约束生成的清晰图像。单幅图像存在的特征信息有限，为了将有限的特征信息得到充分的利用，将生成器改造成了多尺度递归网络，同时在多尺度递归网络中引入密集块替代残差块实现在尺度迭代过程中共享参数，有效减少参数量，降低训练难度，提高网络模型的稳定性。实验结果表明，该网络在单幅图像去模糊中优于其他算法，同时对比成对数据集下的图像去模糊算法，优于现有的 DeblurGAN、DMPHN 等方法。 (2) 研究了一种基于多尺度递归网络的视频去模糊算法。视频去模糊可以看作是多帧图像去模糊。本文利用多尺度递归网络来构建视频去模糊网络，同时融合八度卷积，将视频帧分解为高频及低频部分，分别处理高、低频部分的特征，保证特征信息的充分利用。在低频部分处理上减少计算量，节约资源，降低网络的冗余度。实验结果显示，该算法提升了视频去模糊效果，加快了训练速度。 (3) 设计并实现了一个图像去模糊系统。该系统基于 Web 实现的客户端与服务器端架构，客户端上传模糊图像，服务器端去模糊后返回清晰图像给客户端，工程实现了图像去模糊算法研究。</li>
<li>代码注释：<img src="/assets/4.%E4%BB%A3%E7%A0%81.jpg" alt="代码"><img src="/assets/4.%E4%BB%A3%E7%A0%812.jpg" alt="代码2"><img src="/assets/4.%E4%BB%A3%E7%A0%813.jpg" alt="代码3">
        <h2 id="第六周"   >
          <a href="#第六周" class="heading-link"><i class="fas fa-link"></i></a>第六周</h2>
      </li>
<li>新看过的论文链接<span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD202201&filename=1021875566.nh&uniplatform=NZKPT&v=v6gl0mnqoBpqwsk2cGsyH0KvtfditDkcALiZSLnKMavvCyeRPYxbgjB7lfPvQFim" >https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&amp;dbname=CMFD202201&amp;filename=1021875566.nh&amp;uniplatform=NZKPT&amp;v=v6gl0mnqoBpqwsk2cGsyH0KvtfditDkcALiZSLnKMavvCyeRPYxbgjB7lfPvQFim</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
<li>该论文通过搭建多个深度网络对运动模糊图像进行学习，从而复原出对应的清晰图像。主要研究内容可以分为如下三个部分：（1）为了解决复原模型难以获取全局信息的问题，本文提出了一个全局感知生成对抗网络。通过引入全局上下文模块来获取全局信息，可以使模型更容易处理图像模糊。另外，考虑到模糊图像缺少细节信息，本文还提出了一个空间细节增强模块来自适应地学习特征的空间信息，并对特定位置的信息进行增强，从而使图像细节更清晰。实验结果表明，该方法不仅具有较高的定量评估结果，而且可以很好地去除模糊并恢复图像细节。（2）为了利用不同尺度特征所包含的信息，本文提出了一个多尺度特征融合网络。首先，提出了一个跨尺度特征融合模块来对不同分辨率的特征进行融合，从而提升去模糊性能。另外，还提出了一个多尺度卷积块来获取不同感受野下的局部信息，通过对不同卷积核提取到的特征进行融合，模型可以更好地保留图像细节。最后，本文还在多个尺度上对图像进行重建，使模型可以更准确地预测复原图像。实验结果表明，该方法不仅可以获得更高的性能，而且具有更好的视觉感知效果。（3）为了有效地去除局部区域和全局图像中的模糊，本文结合前两部分工作提出了一个加权空间金字塔特征融合网络。首先，采用空间金字塔的形式将输入图像划分为不同区域，使模型分别学习到局部区域和全局图像的特征，并提出了一个加权特征融合模块来对这些特征中的信息进行融合。另外，考虑到模糊图像中的高频信息退化严重，本文还提出了一种高频增强模块，使图像中的细节更明显。最终的实验结果表明，该方法在定性和定量方面不仅优于前面的两种方法，也优于其他方法。</li>
<li>论文注释：<img src="/assets/4.%E4%BB%A3%E7%A0%814.jpg" alt="代码4"><img src="/assets/4.%E4%BB%A3%E7%A0%815.jpg" alt="代码5">
        <h2 id="第七周"   >
          <a href="#第七周" class="heading-link"><i class="fas fa-link"></i></a>第七周</h2>
      </li>
<li>新看过的论文链接：<span class="exturl"><a class="exturl__link"   target="_blank" rel="noopener" href="https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&dbname=CJFDLAST2017&filename=JSJF201708007&uniplatform=NZKPT&v=VvkRp-Sf1g6Tqsg73aOG_yZedxeYD5YtiTDuvPl8JdxPl9b0SeH5WHzL9gwCTKr-" >https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&amp;dbname=CJFDLAST2017&amp;filename=JSJF201708007&amp;uniplatform=NZKPT&amp;v=VvkRp-Sf1g6Tqsg73aOG_yZedxeYD5YtiTDuvPl8JdxPl9b0SeH5WHzL9gwCTKr-</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></li>
<li>该论文针对基于深度学习的图像去模糊方法无法有效地保留高频纹理信息, 易产生振铃效应，且时间复杂度较高的问题, 提出基于卷积神经网络(CNN)的图像去模糊方法。该方法设计了一种高频信号保持且可快速去模糊的快速 CNN 模型(FCNN)。在此基础上，首先对高频图像进行傅里叶域上的预处理，通过实施傅里叶域去模糊的预处理得到一个初始的清晰图像; 然后将该初始图像小块作为输入，相应的真实清晰图像小块作为标签训练FCNN，得到从模糊图像到潜在清晰图像的映射函数, 实现基于该训练网络的去模糊。定性和定量实验结果表明，文中方法利用 CNN 参数共享的特点，减少了网络训练过程中大量的学习参数；相对前人基于深度学习的去模糊方法，该方法对模糊图像在保持图像纹理细节恢复的同时使计算复杂度得到显著降低。</li>
<li>论文注释：<img src="/assets/4.%E4%BB%A3%E7%A0%816.jpg" alt="代码6"></li>
</ul>
</div><footer class="post-footer"><div class="post-ending ending"><div class="ending__text">------ 本文结束，感谢您的阅读 ------</div></div><div class="post-copyright copyright"><div class="copyright-author"><span class="copyright-author__name">本文作者: </span><span class="copyright-author__value"><a href="http://zhineng1902.github.io">AndiXing LihuaYao YuchenZhou</a></span></div><div class="copyright-link"><span class="copyright-link__name">本文链接: </span><span class="copyright-link__value"><a href="http://zhineng1902.github.io/2022/04/17/%E5%8E%BB%E6%A8%A1%E7%B3%8A/">http://zhineng1902.github.io/2022/04/17/%E5%8E%BB%E6%A8%A1%E7%B3%8A/</a></span></div><div class="copyright-notice"><span class="copyright-notice__name">版权声明: </span><span class="copyright-notice__value">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" rel="external nofollow" target="_blank">BY-NC-SA</a> 许可协议。转载请注明出处！</span></div></div><div class="post-tags"><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="http://zhineng1902.github.io/tags/python/">python</a></span><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="http://zhineng1902.github.io/tags/%E5%8E%BB%E6%A8%A1%E7%B3%8A/">去模糊</a></span><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="http://zhineng1902.github.io/tags/%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2/">傅里叶变换</a></span></div><nav class="post-paginator paginator"><div class="paginator-prev"><a class="paginator-prev__link" href="/2022/04/17/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97/"><span class="paginator-prev__icon"><i class="fas fa-angle-left"></i></span><span class="paginator-prev__text">第五组 时间序列异常检测--Time-series anomaly detection</span></a></div><div class="paginator-next"><a class="paginator-next__link" href="/2022/04/17/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/"><span class="paginator-prev__text">第三组 对比学习--Contrastive Learning</span><span class="paginator-next__icon"><i class="fas fa-angle-right"></i></span></a></div></nav></footer></div></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><div class="sidebar-nav"><span class="sidebar-nav-toc current">文章目录</span><span class="sidebar-nav-ov">站点概览</span></div><section class="sidebar-toc"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%A2%98%E7%9B%AE%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.</span> <span class="toc-text">
          题目介绍</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%A1%B9%E7%9B%AE%E6%88%90%E5%91%98"><span class="toc-number">2.</span> <span class="toc-text">
          项目成员</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%A1%B9%E7%9B%AE%E8%BF%9B%E5%BA%A6"><span class="toc-number">3.</span> <span class="toc-text">
          项目进度</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E5%91%A8"><span class="toc-number">3.1.</span> <span class="toc-text">
          第一周:</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E5%91%A8"><span class="toc-number">3.2.</span> <span class="toc-text">
          第二周:</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E5%91%A8"><span class="toc-number">3.3.</span> <span class="toc-text">
          第三周:</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E5%9B%9B%E5%91%A8"><span class="toc-number">3.4.</span> <span class="toc-text">
          第四周</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%BA%94%E5%91%A8"><span class="toc-number">3.5.</span> <span class="toc-text">
          第五周</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E5%85%AD%E5%91%A8"><span class="toc-number">3.6.</span> <span class="toc-text">
          第六周</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%B8%83%E5%91%A8"><span class="toc-number">3.7.</span> <span class="toc-text">
          第七周</span></a></li></ol></li></ol></section><!-- ov = overview--><section class="sidebar-ov hide"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/assets/avatar.jpg" alt="avatar"></div><p class="sidebar-ov-author__text">这是一个项目实验网站，欢迎浏览学习！</p></div><div class="sidebar-ov-state"><a class="sidebar-ov-state-item sidebar-ov-state-item--posts" href="/archives/"><div class="sidebar-ov-state-item__count">11</div><div class="sidebar-ov-state-item__name">归档</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--categories" href="/categories/"><div class="sidebar-ov-state-item__count">0</div><div class="sidebar-ov-state-item__name">分类</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--tags" href="/tags/"><div class="sidebar-ov-state-item__count">22</div><div class="sidebar-ov-state-item__name">标签</div></a></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="知识共享许可协议" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg"></a></div></section><div class="sidebar-reading"><div class="sidebar-reading-info"><span class="sidebar-reading-info__text">你已阅读了 </span><span class="sidebar-reading-info__num">0</span></div><div class="sidebar-reading-line"></div></div></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2022</span><span class="footer__icon"><i class="fas fa-heart"></i></span><span>AndiXing LihuaYao YuchenZhou</span></div><div><span>由 <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a> 强力驱动</span><span> v5.4.2</span><span class="footer__devider">|</span><span>主题 - <a href="https://github.com/liuyib/hexo-theme-stun/" title="Stun" target="_blank" rel="noopener">Stun</a></span><span> v2.1.1</span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="/js/utils.js?v=2.1.1"></script><script src="/js/stun-boot.js?v=2.1.1"></script><script src="/js/scroll.js?v=2.1.1"></script><script src="/js/header.js?v=2.1.1"></script><script src="/js/sidebar.js?v=2.1.1"></script></body></html>